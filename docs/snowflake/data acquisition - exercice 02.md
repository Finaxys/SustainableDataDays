# Sustainable Data Day 1 : Data Acquisition with Snowflake

## Exercise 2 - Unstructured Data Integration

In the second exercise, we will integrate operational data produced by a market exchange simulator.
Our goal is to manipulate unstructured file format (JSON) and see how we can transform it in order to perform some analysis on market activity.

Let's start by explaining the business concepts that we will manipulate during this session.

## Functional scope of the workshop

A [market place](https://en.wikipedia.org/wiki/Exchange_(organized_market)), market exchange or trading venue is an organized market where securities and other financial instruments are bought and sold.
A market place is organized around a set of **OrderBook**.
**Market participants** place orders to **buy** or **sell** securities.

The exchange records every new order in the order book corresponding to the security in a specific sequence.
Then it tries to **"execute"** the order.
Executing means that the exchange try to find another order that **"match"** in price and in volume the newly placed order. 

Order can be fully or partially executed meaning according to the quantity given for each order.
If an order is fully executed, it is removed from the order book. 
If an order is partially executed, the order is updated with the remaining quantity of share.

When orders are executed, this will generate a new **Price** for the security.

To simulate a market exchange, we are using a R&D project from Lille 1 university call [Atom](https://artificialmarket.univ-lille.fr/)
It is an agent-based simulation of stock-market.
This is a very convenient and efficient way to generate "realistic" financial market activities.

Atom is very modular. It uses the concept of **agent** (representing market participants such as traders, market maker or newsagent), an order-driven **market** microstructure (with order and order books) and a simulation (the dynamic of one or several trading day).
Each agent can have their own behaviour that can be defined programmatically.

We have set up an atom simulation with the following agents:
- Institutional Investors that follow a specific strategy
- Retail investors that don't have strategy
- A Market Maker (mm) that ensure liquidity in the market

We will go into the specific of the strategy of Institutional Investors later.

The main difference between institutional and retail investors is that retail investors allow themselves to lose money.
It is necessary to have a market maker and retail investors in our simulation otherwise there is no activity in the market.

Everything that happen in the market during each trading days is trace in a log file.
The log can become very big so the simulator splits the file regularly.

We will analyse the log to understand what is happening in the market.

The log contains following data types:
- Order trace
- Exec trace
- Price trace
- Agent trace
- Day trace

Each trace has its own format. As everything is written in the same file, the file is not structured. It is semi-structured.
The standard format for market exchange is [FIX](https://www.fixtrading.org/standards/).
We use our own format based on [JSON](https://www.json.org/json-en.html) which is the standard uses for internet application and API.

Now let's review the structure of each trace.

The principal characteristics of an **Order** trace are:
- **SENDER**: the ID of the **agent** which has emitted the order,
- **OBNAME**: the **identifier** of the security (Ticker or ISIN),
- **EXTID**: the order identifier
- **PRICE**: the unitary **price** the agent is willing to pay for the transaction, 
- **QUANTITY**: the **quantity** of share,
- **DIRECTION**: the **direction** of the order (buy=B or sell=S)

The order trace is written every time an agent place an order in the market.

The characteristics of an **Exec** trace are:
- **SENDER**: the ID of the **agent** which has emitted the order,
- **EXTID**: the order identifier

Note: to facilitate analytics, Exec trace also contains all information of the Order

The exec trace is written every time an order is executed by the order book.

The characteristics of a **Price** trace are:
- **OBNAME**: the **identifier** of the security (Ticker or ISIN),
- **PRICE**: the **price** that has been generated by the execution,
- **QUANTITY**: the **quantity** that has been exchanged,
- **DIRECTION**: direction of the order that has generated this price,
- **EXTID1**: order identifier of the order initiating the price,
- **EXTID2**: order identifier of the order fulfilling the other order,
- **BESTASKPRICE**: the current best ask price
- **BESTBIDPRICE**: the current best bid price

The price trace is written every time a new price is generated with an execution.

The characteristics of an **Agent** trace are:
- **NAME**: the agent name
- **CASH**: the agent cash
- **OBNAME**: the identifier of the asset that has been modified
- **INVESTS**: quantity of the asset that has been modified
- **LASTFIXEDPRICE**: current price of the asset that has been modified

The Agent trace is written every time there is a modification in the agent position (ie after the execution of one of his order).

The characteristics of a **Day** trace are:
- **NBDAYS**: day number
- **OBNAME**: asset's name
- **NUMBEROFORDERSRECEIVED**: number of order received
- **NUMBEROFPRICESFIXED**: number of fixed priced
- **FIRSTPRICEOFDAY**: first fixed price of day
- **LASTPRICEOFDAY**: last fixed price of day
- **HIGHESTPRICEOFDAY**: highest fixed price of day
- **LOWESTPRICEOFDAY**: lowest fixed price of day

The Day trace is written for all order books at the end of the trading day.

Every trace also contains the "type" of the trace (Day, Price, Exec, Order, Agent) and a timestamp indicating when the trace occurs in the simulation.
The duration of a simulation is a parameter. We control opening time, closing time of the market and the number of days of the simulation.
So if we run the simulation several times, the timestamps will remain consistent.

To represent the asset's name, we have used **ISIN** code of the corresponding stock.

Time and the sequence of the events are important. Trace represents "Events" that occurs during the simulation.
So if nothing happen, you have no event, so no state to base your analytics on.

Enough with theory.

## Let's get into action

Connect to snowflake and open 'Data Day 1' worksheet.

### On semi-structured data 

As explain above, we will manipulate JSON files which is a semi-structured format. 
Semi-structured means that all data within the file does not have the exact same structure. This is very common. 

Web pages for example contain many different data: header, paragraph, text, image, table, etc.. but there is a global schema (html standard) that allow browsers and search engines to process the data.

Our market exchange log file will contain many traces. As soon as we can identify the type of the trace, we will be able to read it.
This is call parsing data. In JSON, each object is contained with a curly bracket '{}'. Objects are separated by a ','.
Attributes are named and the value is place after a ":". Attributes of the object are separated by a comma ','.

Example: { "attributeText": "value", "attributeNumeric": 1 }. 

This if very basic and so easy to use. One can say it is a little bit verbose (as xml). It has not been conceived for storage or analytics.

When you have many objects in a file, it is also standard to put the data in an outer array to indicate the start and the end of the file. 
Arrays are identified by brackets '[]' in JSON.

Here is an example of a trace log:

```JSON
[
{   "direction": "A",   "extId": "2",   "obName": "FR00103078191",   "orderType": "L",   "price": 4,   "quantity": 100,   "sender": "mm",   "timestamp": 1554447621534,   "type": "Order",   "validity": -1 },
{   "direction": "B",   "extId": "1",   "obName": "FR0010220475",   "orderType": "L",   "price": 5,   "quantity": 0,   "sender": "Denise-01",   "timestamp": 1554453996731,   "type": "Exec",   "validity": 10 },
{   "bestAskPrice": 4,   "bestBidPrice": 18,   "direction": "B",   "extId1": "mm-0",   "extId2": "Idiot-00-203",   "obName": "FR00103078191",   "price": 4,   "quantity": 12,   "timestamp": 1554447621386,   "type": "Price" },
{   "cash": 4108,   "invests": 12,   "lastFixedPrice": 4,   "name": "Idiot-00",   "obName": "FR00103078191",   "timestamp": 1554447621423,   "type": "Agent" },
{   "firstPriceOfDay": 14,   "highestPriceOfDay": 17,   "lastPriceOfDay": 10,   "lowestPriceOfDay": 7,   "nbDays": 1,   "numberOfOrdersReceived": 6044,   "numberOfPricesFixed": 2539,   "obName": "FR00103078191",   "timestamp": 1554447645496,   "type": "Day" }
]
```

We have slightly modified the log by adding a line break after each trace to make it more readable. In real life, json or xml files are not formatted to be readable by human.

As the log is semi-structured, the order of the fields in a trace does not really matter.
But as the log is event based, the order of the trace in the file is. 

While the data platform ingests the file, it splits each trace and store them as text without performing any other operation on it.

To use the data, we need to deserialize the trace in a binary object that computer can manipulate (ie. to give access to information).
To save the binary object in text, it needs to serialize it again in JSON format.
Serializing and deserializing is a very resource intensive, but it needs to be done otherwise the program cannot do anything with the data.

Luckily, Snowflake know how to manipulate semi-structured data and perform all the technical task to "structure" it for us very efficiently.

### Create an file format for semi-structured data

We will first create a file format that handle JSON

```SQL
--- Make sure we are on the right database with the right role and warehouse
USE ROLE DATADAY;
USE DATABASE SUSTAINABLE_DATA_DAYSXY;
USE WAREHOUSE DATADAY_WH; 

-- CREATE A FILE FORMAT FOR ATOM JSON LOG
CREATE OR REPLACE FILE FORMAT atom_json_format_raw
TYPE = 'JSON'
STRIP_OUTER_ARRAY = TRUE;
```

We indicate to Snowflake that it must remove the array in which the traces are enclosed. 

### Using an external stage

In the previous exercise, we have used an internal stage to store RAW data before the integration in a Snowflake table.
The stage was created in Snowflake tenant within the database namespace.

Our market exchange simulator runs in AWS Cloud. The log file is recorded in an AWS S3 bucket.
This would be very convenient that Snowflake load the data directly from AWS so we don't have to copy it in another system.

Good news, it is possible with Snowflake to setup **External Stage**. External stage can be created on AWS, GCP and Azure.

To make things really easy, we have open the access to our S3 bucket so you will be able to create a stage on it directly.
**This is not obviously not a good practice from a security perspective.** 

```SQL
CREATE STAGE S3_ATOM_STAGE
  STORAGE_INTEGRATION = S3_INTEGRATION
  URL = 's3://esg-trader-atom-bucket/'
  FILE_FORMAT = atom_json_format_raw;
```

Again, this work without additional setup because we have made our bucket public for the workshop. Don't do this! 
Snowflake has a very [complete documentation](https://docs.snowflake.com/en/user-guide/data-load-s3) on how to setup external stage on S3.

The trick here is the STORAGE INTEGRATION named 'S3_INTEGRATION' which has been setup by SYSADMIN and granted usage to DATADAY.
If you are on your own Snowflake account, contact us. We will assist you for the setup.

Now you can see the file on the S3 external stage from Snowflake. 

```SQL
LIST @S3_ATOM_STAGE;

```

| name | size | md5 | last_modified |
| --- | --- | --- | --- |
|s3://esg-trader-atom-bucket/20230516202312_atom_01.json	|16,222,005	|2d883e85ab3cd961937e657a8d8d6ce4	|Tue, 16 May 2023 20:23:27 GMT|
|s3://esg-trader-atom-bucket/20230516202312_atom_02.json	|12,500,862	|21d0ec292c24b5c877bbaf5a4af2c5f8	|Tue, 16 May 2023 20:23:36 GMT|
|s3://esg-trader-atom-bucket/20230522180331_atom_01.json	|16,004,545	|4a397f555623cc023188225f6d34f068	|Mon, 22 May 2023 16:03:33 GMT|
| ... | | | |
It's a kind of magic.

### Loading the data from the external stage into a table.

Now that you have access to the file, let's load it in a table.
Snowflake has a very convenient data types for handling semi-structured data. It is call [variant](https://docs.snowflake.com/en/sql-reference/data-types-semistructured). 

We create a 'ATOM_RAW' table with a single attribute Content of types VARIANT and then load data into it.

```SQL
-- Create the ATOM  RAW Table
CREATE OR REPLACE TABLE ATOM_RAW (
	CONTENT VARIANT
);

COPY INTO ATOM_RAW
FROM @S3_ATOM_STAGE
FILE_FORMAT = (FORMAT_NAME = atom_json_format_raw)
PATTERN = '.*20230522180331_.*'
ON_ERROR = 'skip_file';
```


|file|status|rows_parsed|rows_loaded|error_limit|errors_seen|first_error|first_error_line|first_error_character|first_error_column_name|
| --- | --- | --- | --- | --- | --- | -- | --- | --- | --- |
|s3://esg-trader-atom-bucket/20230522180331_atom_07.json|LOADED|100,000|100,000|1|0| |	 | | |			
|s3://esg-trader-atom-bucket/20230522180331_atom_09.json|LOADED|100,000|100,000|1|0| |	 | | |			
| ... | | | | | | | | | |			

That's it. You can run SQL query on the table.

```SQL
SELECT TOP 100 * FROM ATOM_RAW;
```

You will be able to access each trace individually as Snowflake use 1 row per trace.

```JSON
{   "direction": "B",   "extId": "1",   "obName": "FR0000121964",   "orderType": "L",   "price": 18,   "quantity": 55,   "sender": "Beatrice-04",   "timestamp": 1554447600037,   "type": "Order",   "validity": 10 }
```

Ok, but how to use the data inside each row ? This is where the variant data type comes into action.
Snowflake has a semantic so you can make SQL query on the content of variant field!

Let's search the first two Order trace for the ISIN FR0010307819.

```SQL
SELECT TOP 2 
    $1:timestamp::varchar::TIMESTAMP TIMESTAMP,
    $1:obName::STRING OBNAME,
    $1:orderType::STRING ORDERTYPE,
    $1:sender::STRING SENDER,
    $1:extId::STRING EXTID,
    $1:quantity::NUMBER QUANTITY,
    $1:direction::STRING DIRECTION,
    $1:price::NUMBER PRICE,
    $1:validity::NUMBER VALIDITY
FROM
    ATOM_RAW
WHERE
    $1:obName = 'FR0010307819'
    and $1:type = 'Exec';
```

Another query for Price trace:

```SQL
SELECT top 2
    $1:timestamp::varchar::TIMESTAMP TIMESTAMP,
    $1:obName::STRING OBNAME,
    $1:price::NUMBER PRICE,
    $1:quantity::NUMBER QUANTITY,
    $1:extId1::STRING EXTID1,
    $1:extId2::STRING EXTID2,
    $1:bestAskPrice::NUMBER BESTASKPRICE,
    $1:bestBidPrice::NUMBER BESTBIDPRICE
FROM
    ATOM_RAW
WHERE
    $1:obName = 'FR0010307819'
    AND $1:type = 'Price';
```
For more details, read the documentation on [how to query semi-structured data](https://docs.snowflake.com/en/user-guide/querying-semistructured).

### Structuring variant for good.
Even if querying semi-structured data directly in SQL is a very cool feature especially for discovery data, we would like structure the information for good.

As many database management system, it is  possible with snowflake to create a **'View'** with query, so it looks like a table.
This is a very useful especially if you want to share a query and add some access control on it.

Note that snowflake also supports [Common Table Expression](https://docs.snowflake.com/en/user-guide/queries-cte) (CTE) which is very powerful to express complex queries.

Let's structure our trace for good.

```SQL
CREATE or REPLACE VIEW ATOM_EXEC_VIEW
    as
    (SELECT
        $1:timestamp::varchar::TIMESTAMP TIMESTAMP,
        $1:obName::STRING OBNAME,
        $1:orderType::STRING ORDERTYPE,
        $1:sender::STRING SENDER,
        $1:extId::STRING EXTID,
        $1:quantity::NUMBER QUANTITY,
        $1:direction::STRING DIRECTION,
        $1:price::NUMBER PRICE,
        $1:validity::NUMBER VALIDITY
    FROM
        ATOM_RAW e
    WHERE
        $1:type = 'Exec'
    );
    
    SELECT TOP 10 OBNAME, ORDERTYPE, SENDER, QUANTITY, DIRECTION, PRICE, VALIDITY, TIMESTAMP FROM ATOM_EXEC_VIEW WHERE OBNAME = 'FR0010307819';
```

Now you can use exact SQL semantic to query Exec trace. As the filtering on 'Exec' type is part of the view 'from' request, it is systematically applied to all query.

### On Snowflake views
There is 4 types of views in Snowflake: normal view as the one we have just created but also materialized view and secure views.

On normal view, the query is performed each time you run a request on it.
If the query is complex, the query can take some time to be executed on big data set.

In this case, you can prefer to use a materialized view. Snowflake will automatically create a table that represent the result of the view query and update the content of the table every time the underlying data is change.
The query on the view will be way faster (no recompute), but if the data change often, Snowflake will start to consume more credits to update.

So choose wisely the type of view you create.

Then a view (materialized or not materialized) can be secure or not. Which means the query of the view is hidden to the user.
It could be useful if you don't want people to know which tables are used by the query. 
Secure views are mandatory if you are using data sharing which make a lot of sense as you don't want your customer to see how you manage the data you share with them.

### Practice, Let's make some analytics

Now that we can run structured query on JSON data, we can join with other reference table and  perform some analytics.

We will want to see the quantity of share placed every hour the 5th per sector and graph it in a bar chart.

You will be on your own with just little tips:
- create a view ATOM_ORDER_VIEW on Order trace from ATOM_RAW table
- join ATOM_ORDER_VIEW with INVESTMENT_UNIVERSE (obname is the ISIN code of the traded asset)
- join INVESTMENT_UNIVERSE with ISSUERS_SECTORS (the issuer is classified by sector)
- select the 5 day of the month with DATE_PART( 'DAY' , timestamp ) = 5 
- don't forget to sum by quantity and group by sector and hour

As we are nice guys, here are the attributes of 'Order' trace

```SQL
        $1:timestamp::varchar::TIMESTAMP TIMESTAMP,
        $1:obName::STRING OBNAME,
        $1:orderType::STRING ORDERTYPE,
        $1:sender::STRING SENDER,
        $1:extId::STRING EXTID,
        $1:quantity::NUMBER QUANTITY,
        $1:direction::STRING DIRECTION,
        $1:price::NUMBER PRICE,
        $1:validity::NUMBER VALIDITY'
```

For the Chart :
- Chart type: Bar
- Data: 
  - Column QUANTITY use as Bar aggregation Sum
  - Column SECTOR_CODE use as Series 
  - Column HOUR use as X-Axis bucketing None
- Appearance
  - Orientation: Vertical
  - Grouping: Stacked
  - No Stretching
  - ORder bars by HOUR
  - Order direction Ascending
  - Series direction Ascending
  - Show legend yes
  - Label X-Axis : HOUR
  - Label Y-AXis Quantity

This is the graph we expect to have:

![Placed quantity by sector per hour](res/Data Day 2.png)

## Wrapping up

During this second exercise, we have discovered basic elements of data project:
- **External Stage**: getting access to cloud storage to load data into snowflake;
- **JSON and semi-structured file format**: the world is not flat and so is the data to ingest
- **Variant type**: storing and manipulating semi-structured data with snowflake;
- **View**: queries that can be used like table;

There is 1,762,584 rows in the dataset we play with today with the smallest warehouse size Snowflake offer.
Uploading data to analytics, everything ran smoothly.
